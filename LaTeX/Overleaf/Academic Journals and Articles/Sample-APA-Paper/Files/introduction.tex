%\section{Introduction}

Petri nets are a family of mathematical modeling languages with a well-\-established visual representation.
They are used both in academia \cite{desel2001petri} and in the private sector \cite{desrochers1995applications} in different variations.
The first specification goes back to \textcite{carl1962petri} who focused on the development of an asynchronous automaton model.
Since then the field has largely diverged \cite{desel2001petri}. 

Since some professionals work with Petri nets on a frequent basis, it can be expected that those develop a different, more advanced perception (in its extended sense) for their domain, meaning that they can remember and recall larger fragments of domain knowledge with a higher accuracy compared to lays of the respective field.
Such findings exist for many domains, the most related domains in terms of visual material similarity might be the technical drawings of electrical engineers \cite{egan1979chunking} and mechanical engineers \cite{moss2006role}.
But the findings in the field of expertise research go far beyond that and include domains such as programming \cite{sheinderman1976exploratory}, music \cite{sloboda1976music} and waitressing \cite{ericsson1988experimental};
a more extensive list has been compiled by \textcite{oulasvirta2006surviving}.
The reported differences vary between studies:
In chess experts simply outperform lays in recalling chess positions \cite{gobet1996templates} whereas in domains like map-reading lays recall different aspects of the maps -- aspects which are actually missing in the experts' recall \cite{gilhooly1988skill}.
Lays are mostly guided by visual properties of the material \cite{canham2010effects,moss2006role,egan1979chunking} -- an observation which has been named \textit{perceptual chunking hypothesis} \cite{simon1973simulation}.
However, this hypothesis does not work for experts because experts use their knowledge about the domain to form chunks which are guided by functionality \cite{moss2006role,egan1979chunking}.
As an example, \citeA{moss2006role} examined for what guided freshmen and seniors in forming their chunks.
Freshmen were guided by the principle of proximity:
Whichever elements were close to each other were retained as one unit even if those elements did not perform a common task or were not connected.
Seniors instead recalled units ordered from input to output and drew each unit together with its functionally related units.

While some of the above-mentioned studies solely measured how lays and experts differ, others also introduced and justified certain theoretical models which account for the performance differences.
Performance is used to describe the error rates at the end of the recall and dynamics during the recall, namely the breaks which occur between the recall of the different units.
For these units the term \textit{chunk} has been coined by \textcite{miller1956magical} who differentiates between \textit{bits} and \textit{chunks}.
While bits describe each single piece of information, chunks are an agglomeration of bits and according to \citeA{miller1956magical} only about 7 of them can be kept in short-term memory.
While this number is too optimistic for meaningless visual material for which an accurate repetition was only possible for 4 items \cite{luck1997capacity}, chess masters have greatly outperformed that number by placing up to 178 pieces correctly \cite{gobet1996templates}.
Whatever strategy had been used, it seems rather na\"{\i}ve to believe that this was possible by having only 7 Miller chunks available, at some point they assume that one participant formed up to 15 chunks.
\textcite{gobet1996templates} differentiate in their model between clusters which are derived from the chunks of \textcite{chase1973mind} and describe familiar patterns within the material and templates which have some slots in which chunks can be filled in.
By nesting clusters in templates, the chunking (from now on both clusters and templates are referred to as chunks) turns hierarchical.
Empiric evidence further comes from \textcite{moss2006role}.
But chunks do not only nest, they also overlap \cite{reitman1976skilled}:
Some chunks on a low hierarchical level are part of several higher-level chunks.
Methodologically chunks have been identified by performance measures.
Quite often Inter-Response-Time (IRT) analyses have been performed \parencites(e.g.,)(){egan1979chunking,reitman1976skilled} which was initially proposed by \textcite{chase1973perception}.
Another source of information has been the switch of looking at the source and the target during a copying process \cite{reitman1976skilled} and comparing the chance of making errors at certain elements during the recall phase \cite{moss2006role}.

The most common approach in expertise literature is the immediate recall of the material after a short presentation time.
The recall condition is especially interesting because of its dynamics and the richness of errors which happen during the process and which can give many hints about the underlying processes.
Well-known effects like the primacy and recency effect have been found \cite{gobet1996templates} and a classification of errors into error of omission and error of commission \cite{chase1973mind} helped to understand the differences between lays and experts better.
The short presentation time prevents the participants to learn the material.
Empiric findings of verbal learning tasks show that it takes approximately 10 seconds to transfer one chunk from the working memory to long term memory \cite{newell1972human}.
While the exact number of seconds might vary \parencites(cf. e.g.,)(){egan1979chunking}, longer time spans like e.g. 40 seconds make error score differences between lays and experts disappear \cite{moss2006role}.
The experts' advantage is grounded in that once a chunk is stored in the long term memory, it can be recognized and utilized within much shorter time spans \parencites(e.g.,)(){gobet1996templates}.
This altogether is the framework in which expertise has been mostly dealt with.
Long term memory is mainly regarded as the expertise database for recognizing chunks.
The theory of long-term working memory elaborates on the contribution and nature of the long term memory to the processes which are traditionally attributed to working memory \cite{ericsson1996expert,ericsson1999long}.
Once chunks are transferred to long-term working memory, which can take as little as 200ms \cite{ericsson1999long} but can go up to 2s \cite{ericsson1988experimental} for more complex chunks, they are virtually safe-guarded \cite{oulasvirta2006surviving,gobet1996templates}.
This nature of long-term working memory allows experts to continue their work after interruption without any information loss under most conditions \cite{oulasvirta2006surviving}.
Lays by contrast suffer much from such an interruption because they lack of appropriate chunks in long-term memory.
Hence most task-related memories are irrecoverably lost after task interruption
\cite{oulasvirta2006surviving}.
